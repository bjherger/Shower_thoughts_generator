import datetime
import logging
import os
import tempfile

import numpy
import pandas
import yaml

import numpy as np

# Global variables
from sklearn.preprocessing import LabelEncoder

import main

CONFS = None
BATCH_NAME = None
TEMP_DIR = None


def load_confs(confs_path='../conf/conf.yaml'):
    """
    Load configurations from file.

     - If configuration file is available, load it
     - If configuraiton file is not available attempt to load configuration template

    Configurations are never explicitly validated.
    :param confs_path: Path to a configuration file, appropriately formatted for this application
    :type confs_path: str
    :return: Python native object, containing configuration names and values
    :rtype: dict
    """
    global CONFS

    if CONFS is None:

        try:
            logging.info('Attempting to load conf from path: {}'.format(confs_path))

            # Attempt to load conf from confPath
            CONFS = yaml.load(open(confs_path))

        except IOError:
            logging.warn('Unable to open user conf file. Attempting to run with default values from conf template')

            # Attempt to load conf from template path
            template_path = confs_path + '.template'
            CONFS = yaml.load(open(template_path))

    return CONFS


def get_conf(conf_name):
    """
    Get a configuration parameter by its name
    :param conf_name: Name of a configuration parameter
    :type conf_name: str
    :return: Value for that conf (no specific type information available)
    """
    return load_confs()[conf_name]


def get_batch_name():
    """
    Get the name of the current run. This is a unique identifier for each run of this application
    :return: The name of the current run. This is a unique identifier for each run of this application
    :rtype: str
    """
    global BATCH_NAME

    if BATCH_NAME is None:
        logging.info('Batch name not yet set. Setting batch name.')
        BATCH_NAME = str(datetime.datetime.utcnow()).replace(' ', '_').replace('/', '_').replace(':', '_')
        logging.info('Batch name: {}'.format(BATCH_NAME))
    return BATCH_NAME


def get_temp_dir():
    global TEMP_DIR
    if TEMP_DIR is None:
        TEMP_DIR = tempfile.mkdtemp(prefix='reddit_')
        logging.info('Created temporary directory: {}'.format(TEMP_DIR))
        print('Created temporary directory: {}'.format(TEMP_DIR))
    return TEMP_DIR


def archive_dataset_schemas(step_name, local_dict, global_dict):
    """
    Archive the schema for all available Pandas DataFrames

     - Determine which objects in namespace are Pandas DataFrames
     - Pull schema for all available Pandas DataFrames
     - Write schemas to file

    :param step_name: The name of the current operation (e.g. `extract`, `transform`, `model` or `load`
    :param local_dict: A dictionary containing mappings from variable name to objects. This is usually generated by
    calling `locals`
    :type local_dict: dict
    :param global_dict: A dictionary containing mappings from variable name to objects. This is usually generated by
    calling `globals`
    :type global_dict: dict
    :return: None
    :rtype: None
    """
    logging.info('Archiving data set schema(s) for step name: {}'.format(step_name))

    # Reference variables
    data_schema_dir = get_conf('data_schema_dir')
    schema_output_path = os.path.join(data_schema_dir, step_name + '.csv')
    schema_agg = list()

    env_variables = dict()
    env_variables.update(local_dict)
    env_variables.update(global_dict)

    # Filter down to Pandas DataFrames
    data_sets = filter(lambda (k, v): type(v) == pandas.DataFrame, env_variables.iteritems())
    data_sets = dict(data_sets)

    header = pandas.DataFrame(columns=['variable', 'type', 'data_set'])
    schema_agg.append(header)

    for (data_set_name, data_set) in data_sets.iteritems():
        # Extract variable names
        logging.info('Working data_set: {}'.format(data_set_name))

        local_schema_df = pandas.DataFrame(data_set.dtypes, columns=['type'])
        local_schema_df['data_set'] = data_set_name

        schema_agg.append(local_schema_df)

    # Aggregate schema list into one data frame
    agg_schema_df = pandas.concat(schema_agg)

    # Write to file
    agg_schema_df.to_csv(schema_output_path, index_label='variable')

def legal_characters():
    return set("""1234567890,.abcdefghijklmnopqrstuvwxyz ;?!&-""")

def find_ngrams(input_list, n):
    return zip(*[input_list[i:] for i in range(n)])

def model_predict(encoder, ohe, model, text):

    # Transform characters
    chars = list(text)
    chars, encoded_chars, new_encoder, X = main.transform(chars)

    # Infer y hat value
    pred = model.predict(X)[-1]

    # Scale y hat so that it is in [0,1)
    scaled_pred = pred / (float(sum(pred)) * (1 + 1e-5))
    logging.debug('Sum of pred: {}'.format(float(sum(pred))))

    # Pick the most likely character by index
    most_likely_index = numpy.argmax(scaled_pred)

    # Pick a character from multinomial
    next_char_index = numpy.argmax(numpy.random.multinomial(1, scaled_pred, 1))

    # Convert both most likely and multinomial random from index to character
    most_likely_char = encoder.inverse_transform(most_likely_index)
    next_char = encoder.inverse_transform(next_char_index)
    logging.info('Most likely char, p: {}, {}. Next char, p: {}, {}'.format(most_likely_char, pred[most_likely_index],
                                                                            next_char, pred[next_char_index]))

    # Return multinomial random character
    return next_char


def get_char_indices():
    chars = sorted(list(set(legal_characters())))
    return dict((c, i) for i, c in enumerate(chars))

def get_indices_char():
    chars = sorted(list(set(legal_characters())))
    return dict((i, c) for i, c in enumerate(chars))


def finish_sentence(encoder, ohe, model, text, num_chars=100):
    result_string = text

    while len(result_string) < len(text) + num_chars:
        pred_char = model_predict(encoder, ohe, model, result_string)
        result_string += pred_char

    result_string=result_string[len(text):]
    return result_string

def gen_x_y(text, false_y):
    chars = sorted(list(set(legal_characters())))
    char_indices = get_char_indices()
    indices_char = get_indices_char()
    sentences = list()
    next_chars = list()
    step = 3


    if false_y:
        text +=' '

    text = map(lambda x: x.lower(), text)
    text = map(lambda x: x if x in legal_characters() else ' ', text)
    text = ''.join(text)
    text = ' '.join(text.split())

    # Cut the text in semi-redundant sequences of maxlen characters
    for observation_index in range(0, len(text) - get_conf('ngram_len'), step):
        sentences.append(text[observation_index: observation_index + get_conf('ngram_len')])
        next_chars.append(text[observation_index + get_conf('ngram_len')])

    # Convert all sequences into X and Y matrices
    x = np.zeros((len(sentences), get_conf('ngram_len')))
    y = np.zeros((len(sentences), len(chars)), dtype=bool)

    for observation_index, sentence in enumerate(sentences):
        for t, char in enumerate(sentence):
            x[observation_index, t] = char_indices[char]
        y[observation_index, char_indices[next_chars[observation_index]]] = 1

    return x, y
